{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 03\n",
    "## Lars KÃ¼hmichel, Nicolas Wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scistats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", dpi=150)\n",
    "plt.rc(\"legend\", fontsize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1: Univariate AR Models\n",
    "\n",
    "Consider the time series `DLPFC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As always, load and inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ex3file1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DLPFC</th>\n",
       "      <th>DLPFC.1</th>\n",
       "      <th>Parietal</th>\n",
       "      <th>Parietal.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.28550</td>\n",
       "      <td>-0.29516</td>\n",
       "      <td>-3.0362</td>\n",
       "      <td>-4.51840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.85457</td>\n",
       "      <td>-2.63380</td>\n",
       "      <td>-4.5223</td>\n",
       "      <td>-1.90250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.26151</td>\n",
       "      <td>-3.08110</td>\n",
       "      <td>-4.7146</td>\n",
       "      <td>0.47887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.19410</td>\n",
       "      <td>-2.62330</td>\n",
       "      <td>-3.9754</td>\n",
       "      <td>1.92820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.27900</td>\n",
       "      <td>-1.61420</td>\n",
       "      <td>-2.0104</td>\n",
       "      <td>3.77230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     DLPFC  DLPFC.1  Parietal  Parietal.1\n",
       "0  2.28550 -0.29516   -3.0362    -4.51840\n",
       "1  0.85457 -2.63380   -4.5223    -1.90250\n",
       "2 -0.26151 -3.08110   -4.7146     0.47887\n",
       "3 -1.19410 -2.62330   -3.9754     1.92820\n",
       "4 -1.27900 -1.61420   -2.0104     3.77230"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"DLPFC\"].plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Compute the log-likelihood of an AR(4) model. Please write down explicitly.\n",
    "\n",
    "According to the script, the log-likelihood is given as:\n",
    "\n",
    "$$\n",
    "\\log L(\\{ \\alpha_i \\}, \\sigma) = - \\frac{T-p}{2} \\log (2 \\pi) - \\frac{T-p}{2} \\log(\\sigma^2) - \\frac{1}{2} \\underline{\\varepsilon}^T \\underline{\\varepsilon} \\sigma^{-2}\n",
    "$$\n",
    "\n",
    "where $\\{ \\alpha_i \\}$ is the set of model coefficients, $\\sigma$ is the standard deviation of the process noise $\\underline{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\underline{I})$, $T$ is the length of the time series, and $p$ is the order of the AR process.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = sm.tsa.AutoReg(endog=df[\"DLPFC\"], lags=4, trend=\"n\")\n",
    "result = model.fit()\n",
    "\n",
    "p = 4\n",
    "T = len(df)\n",
    "sigma = np.sqrt(result.sigma2)\n",
    "epsilon = result.resid\n",
    "ll = -0.5 * (T - p) * np.log(2 * np.pi) - 0.5 * (T - p) * np.log(sigma ** 2) - 0.5 * np.dot(epsilon, epsilon) * sigma ** (-2)\n",
    "llf = result.llf\n",
    "\n",
    "print(f\"Library: {llf:.2f}\")\n",
    "print(f\"Ours:    {ll:.2f}\")\n",
    "print(f\"Close?   {np.isclose(llf, ll)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The log-likelihood is quite small. However, the model yields a good fit:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"DLPFC\"].plot()\n",
    "sns.lineplot(x=df.index, y=result.predict(), label=\"AR(4) Fit\")\n",
    "result.forecast(steps=30).plot(color=\"C1\", ls=\":\", alpha=0.75, label=\"AR(4) Forecast\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Plot the residuals of the model in a histogram. What do they look like? What do you expect?\n",
    "\n",
    "The model still deviates from the true signal especially at the peaks. Thus, we would expect significant residuals in those areas. Residuals should look like the multivariate normal noise $\\underline{\\varepsilon}$, which we confirm with the histogram and an overlaid distribution kde plot. Notably, the residuals are not time-dependent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "residuals = df[\"DLPFC\"] - result.predict()\n",
    "sns.lineplot(x=df.index[model.hold_back:], y=result.resid)\n",
    "plt.title(\"Residuals\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(data=residuals, stat=\"density\")\n",
    "sns.kdeplot(data=residuals, fill=True, cut=0, label=\"Observed\", ax=plt.gca())\n",
    "x = np.linspace(*plt.xlim(), num=100)\n",
    "y = scistats.norm.pdf(x, scale=sigma)\n",
    "plt.plot(x, y, ls=\"--\", alpha=0.5, color=\"black\", label=\"Theory\")\n",
    "plt.xlabel(r\"$\\varepsilon$\")\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Compute the log-likelihood of an AR($n$) model, with $n$ ranging from $1...5$.\n",
    "\n",
    "We compute it from $1...100$:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ns = np.arange(start=1, stop=100 + 1)\n",
    "\n",
    "results = []\n",
    "lls = []\n",
    "\n",
    "for n in ns:\n",
    "    model = sm.tsa.AutoReg(endog=df[\"DLPFC\"], lags=n, trend=\"n\")\n",
    "    result = model.fit()\n",
    "    results.append(result)\n",
    "    lls.append(result.llf)\n",
    "\n",
    "likelihoods = pd.DataFrame({\"log-likelihood\": lls}, index=ns)\n",
    "\n",
    "likelihoods.plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How does the likelihood change when you increase the order of the model?\n",
    "\n",
    "The log-likelihood increases as the number of parameters increases. This means the model will more accurately capture the training set, which is logical since the model gains explanatory power. However, a large number of parameters will lead to overfitting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Bonus Exercise: Increasing the capacity of the model is likely to increase its explanatory power, but it is important to explore the tradeoff between this and the increase in model parameters. Determine the optimal order $p$ of the AR model by computing the log-likelihood ratio test statistic.\n",
    "\n",
    "The p-value for the likelihood-ratio-test gives the probability that the hypothesis \"The unrestricted model improves on the restricted one\" is false. The optimal order is thus that of the restricted model at the first (desired) maximum of p."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.base.model import LikelihoodModelResults as LMR\n",
    "\n",
    "\n",
    "def lr_test(restricted: LMR, unrestricted: LMR) -> float:\n",
    "    \"\"\" Perform a Likelihood-Ratio-Test between a restricted and unrestricted model \"\"\"\n",
    "    # determine the appropriate degrees of freedom\n",
    "    dof = restricted.df_resid - unrestricted.df_resid\n",
    "    # likelihood-ratio-test-statistic or \"D\" on the sheet:\n",
    "    stat_lr = -2 * (restricted.llf - unrestricted.llf)\n",
    "    # perform chi-squared test (compute p-value)\n",
    "    return scistats.chi2.sf(stat_lr, df=dof)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimal order is 2. An argument could also be made for 20, 30, 63, 71 and 98 since these have even higher p-values, depending on the statistical certainty requirement. Orders 28, 29 and 30 also stand out because they successively yield good p-values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ps = []\n",
    "\n",
    "for restricted, full in zip(results[:-1], results[1:]):\n",
    "    ps.append(lr_test(restricted, full))\n",
    "\n",
    "peaks = []\n",
    "m = ps[0]\n",
    "for i, p in enumerate(ps):\n",
    "    if p > m:\n",
    "        m = p\n",
    "        peaks.append(i + 1)\n",
    "\n",
    "\n",
    "sns.lineplot(x=ns[:-1], y=ps, label=\"p-Values\")\n",
    "print(f\"Marked Peaks are at: {peaks}\")\n",
    "for p in peaks:\n",
    "    plt.axvline(p, ls=\":\", lw=1, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Order of Restricted Model\")\n",
    "plt.ylabel(\"p\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Likelihood-Ratio-Test Results\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Multivariate (vector) AR (=VAR) processes\n",
    "\n",
    "Now use all four time series `DLPFC1, DLPFC2, Parietal1, Parietal2`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=\"all\", sharey=\"all\")\n",
    "df[[\"DLPFC\", \"DLPFC.1\"]].plot(ax=axes[0])\n",
    "df[[\"Parietal\", \"Parietal.1\"]].plot(ax=axes[1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 1. Estimate a VAR(1) model by performing multivariate regression on the 4-variate time series."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = sm.tsa.VAR(df)\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What do the coefficients in matrix $A$ tell you about the coupling between the DLPFC and parietal cortex?\n",
    "\n",
    "The `DLPFC` and `Parietal` series are weakly coupled, since the coupling coefficients are `0.016` and `-0.012` respectively. We can also reason that the absolute influence `DLPFC` has on `Parietal` is around the same strength as the other way around, i.e. neither is particularly causative of the other. The strongest (non-self) coupling exists from `Parietal.1` to `DLPFC`, with a coefficient of `0.154`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "names = [\"DLPFC\", \"DLPFC.1\", \"Parietal\", \"Parietal.1\"]\n",
    "A = pd.DataFrame(data=np.squeeze(result.coefs), index=names, columns=names)\n",
    "A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Is the resulting VAR(1) model stationary or not?\n",
    "\n",
    "We would expect the model to be stationary, since the underlying time series were stationary. Since the model fits well, this is a reasonable assumption."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = np.array(df.iloc[-1])[None, :]\n",
    "in_sample = model.predict(result.params)\n",
    "in_sample = pd.DataFrame(data=in_sample, index=df.index[1:], columns=df.columns)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, sharex=\"all\", sharey=\"all\", figsize=(16, 4))\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    ax = axes[i]\n",
    "    df[col].plot(ax=ax, label=\"True\")\n",
    "    in_sample[col].plot(ax=ax, label=\"Predicted\")\n",
    "    ax.set_title(col)\n",
    "    ax.legend()\n",
    "\n",
    "fig.text(0.5, 0.01, \"Time\", ha=\"center\", va=\"top\")\n",
    "fig.text(0.01, 0.5, \"Time Series\", rotation=\"vertical\", va=\"center\", ha=\"right\")\n",
    "fig.suptitle(\"VAR(1) Fits\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: AR Poisson Processes\n",
    "\n",
    "### 1. Create your own second order Poisson time series with the given parameters.\n",
    "\n",
    "According to equation 7.36 in the script:\n",
    "$$\n",
    "\\log \\underline{\\mu}_t = \\underline{a}_0 + \\sum_{m=1}^{M} \\underline{A}_m \\underline{c}_{t-m}\n",
    "$$\n",
    "\n",
    "With $c_{it} \\sim \\operatorname{Poisson}(\\mu_{it}) \\quad \\forall i$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_AR_poisson_data(T, mu0, As, a0):\n",
    "    mu = mu0\n",
    "    c_arr = np.zeros(shape=(T, len(a0)))\n",
    "    c_arr[0] = np.random.poisson(mu, 2)\n",
    "    \n",
    "    M = len(As)\n",
    "    for t in range(1, T):\n",
    "        log_mu_t = a0.copy()\n",
    "        for m in range(1, min(M, t)+1):\n",
    "            log_mu_t += As[m-1].dot(c_arr[t-m])\n",
    "        mu = np.exp(log_mu_t)\n",
    "        c_arr[t] = np.random.poisson(mu, 2)\n",
    "        \n",
    "    return c_arr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T = 1000\n",
    "A1 = np.array([\n",
    "    [0.2, -0.1],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "A2 = np.array([\n",
    "    [0.1, -0.1],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "mu0 = np.array([0.5, 0.5]).T\n",
    "a0 = np.array([0.0, 0.0]).T\n",
    "ts = create_AR_poisson_data(T, mu0, [A1, A2], a0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(ts[:,0], label=r\"$c_1$\")\n",
    "plt.plot(ts[:,1], label=r\"$c_2$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"c\")\n",
    "plt.legend()\n",
    "plt.title(\"2nd order AR-Poisson time series\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(data=ts, stat=\"density\")\n",
    "sns.kdeplot(data=ts, fill=True, bw_adjust=3, cut=0, ax=plt.gca())\n",
    "plt.legend(labels=[r\"$c_1$\", r\"$c_2$\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Given the data generated in (1), vary the parameters $A_1(1, 1)$ and $A_2(2, 1)$ between 0 and 0.4 with 0.01 increments. For each parameter value pair, compute the log-likelihood of the data (keeping all other parameters fixed).\n",
    "\n",
    "The log likelihood for a poisson process is given by equation (7.37) in the script:\n",
    "$$\n",
    "\\log p(\\{\\underline{c}_t\\} | \\underline{A}) = \\log\\left[\\prod_{t=M+1}^{T}\\prod_{i=1}^{p}\\frac{\\mu_{it}^{c_{it}}}{c_{it}!}e^{-\\mu_{it}}\\right] = \\sum_{t=M+1}^{T}\\sum_{i=1}^{p} c_{it}\\log\\mu_{it}-\\mu_{it}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_likelihood_AR_Poisson(data, As, a0):\n",
    "    T = len(data)\n",
    "    M = len(As)\n",
    "    ll = 0\n",
    "    mu_arr = np.zeros(shape=data.shape)\n",
    "    for t in range(M, T):\n",
    "        log_mu_t = a0.copy()\n",
    "        for m in range(1, min(t, M)+1):\n",
    "            log_mu_t += As[m-1].dot(data[t-m])\n",
    "        \n",
    "        ll += np.sum(data[t]*log_mu_t-np.exp(log_mu_t))\n",
    "\n",
    "        \n",
    "    return ll"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameter_range = np.linspace(0, 0.4, int(0.4/0.01)+1)\n",
    "lls = np.zeros(shape=(len(parameter_range), len(parameter_range)))\n",
    "for i, a1 in enumerate(parameter_range):\n",
    "    for j, a2 in enumerate(parameter_range):\n",
    "        A1 = np.array([\n",
    "            [a1, -0.1],\n",
    "            [0.1, 0.1]\n",
    "        ])\n",
    "        A2 = np.array([\n",
    "            [0.1, -0.1],\n",
    "            [a2, 0.1]\n",
    "        ])\n",
    "        lls[i,j] = log_likelihood_AR_Poisson(ts, [A1, A2], a0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "argmax_indices = np.unravel_index(lls.argmax(), lls.shape)\n",
    "print(argmax_indices, (parameter_range[argmax_indices[0]], parameter_range[argmax_indices[1]]), lls[argmax_indices])\n",
    "print((20,10), (parameter_range[20], parameter_range[10]), lls[20,10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the log-likelihood landscape surface as a function of these two parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "plt.imshow(lls.T, cmap='hot', interpolation='nearest', extent=[0,0.4,0,.4], origin='lower')\n",
    "plt.colorbar()\n",
    "plt.scatter(parameter_range[argmax_indices[0]], parameter_range[argmax_indices[1]], label=\"argmax\")\n",
    "plt.scatter(0.2, 0.1, label=\"real parameter\")\n",
    "plt.xlabel(r\"$A_1(1,1)$\")\n",
    "plt.ylabel(r\"$A_2(2,1)$\")\n",
    "plt.legend()\n",
    "plt.title(\"Log-likelihood landscape\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Does the real parameter pair value correspond (or is close) to an extreme point in the approximate log-likelihood landscape? What kind of extreme point is it?\n",
    "\n",
    "The extreme point of L($A_1(1,1)$, $A_2(2,1)$) is at $(0.18, 0.11)$ which is very close to the real parameters $(0.2, 0.1)$. Since this extreme point is a maximum, the argmax parameters are the parameters for which the real data are most probable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Does the real parameter pair value correspond (or is close) to an extreme point in the approximate log-likelihood landscape? What kind of extreme point is it?\n",
    "\n",
    "The extreme point of L($A_1(1,1)$, $A_2(2,1)$) is at $(0.18, 0.11)$ which is very close to the real parameters $(0.2, 0.1)$. Since this extreme point is a maximum, the argmax parameters are the parameters for which the real data are most probable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}